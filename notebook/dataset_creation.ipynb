{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to python: run `jupyter nbconvert --to python dataset_creation.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import wget\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import IPython\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Make sure to change these configs before running the whole notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Base URL: The BackBlaze dataset URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Base Path: This should be pointing to where to hold the dataset.  \n",
    "Note that this relative path is relative to the current working directory, or in other words, usually where this ipynb is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "base_path = os.path.abspath(os.path.join(notebook_path, '..', 'HDD_dataset'))\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output Path: This should be pointing to where to output the database, used by the `Classification.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = os.path.abspath(os.path.join(notebook_path, '..', 'output'))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Years: Years of data to download and analyze (From 2013 to 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(year) for year in range(2013, 2020)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model: The specific HDD model we want to keep the data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"ST3000DM001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find Failed: if `True`, keep only failed HDDs, otherwise keep all HDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_failed = False\n",
    "suffix = 'failed' if find_failed else 'all'\n",
    "suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define variables for the name of the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories for each year\n",
    "year_dirs = {year: os.path.join(base_path, year) for year in years}\n",
    "years_list = \"_\" + \"_\".join(years)\n",
    "years_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zips contain different directory names or no directory at all, which causes unavoidable \"spaghettiness\" in the code.\n",
    "For example, the data in year 2013 is in the directory \"2013\", so the key is `\"2013\"`. The data from 2016 are in the root of the zip file, hence the key is `None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zips contain different directory names or no directory at all, which causes\n",
    "# unavoidable \"spaghettiness\" in the code\n",
    "suffixes = {\n",
    "    \"data_2013.zip\": '2013',\n",
    "    \"data_2014.zip\": '2014',\n",
    "    \"data_2015.zip\": '2015',\n",
    "    \"data_Q1_2016.zip\": None,\n",
    "    \"data_Q2_2016.zip\": None,\n",
    "    \"data_Q3_2016.zip\": None,\n",
    "    \"data_Q4_2016.zip\": None,\n",
    "    \"data_Q1_2017.zip\": None,\n",
    "    \"data_Q2_2017.zip\": None,\n",
    "    \"data_Q3_2017.zip\": None,\n",
    "    \"data_Q4_2017.zip\": None,\n",
    "    \"data_Q1_2018.zip\": None,\n",
    "    \"data_Q2_2018.zip\": None,\n",
    "    \"data_Q3_2018.zip\": None,\n",
    "    \"data_Q4_2018.zip\": None,\n",
    "    \"data_Q1_2019.zip\": None,\n",
    "    \"data_Q2_2019.zip\": None,\n",
    "    \"data_Q3_2019.zip\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip the dataset, moving files to the correct directory\n",
    "The dataset is structured as follows:\n",
    "```\n",
    "base_path  \n",
    "├── 2018\n",
    "│   ├── 2018-01-01.csv\n",
    "│   ├── 2018-01-02.csv\n",
    "│   └── 2018-01-03.csv\n",
    "├── 2019\n",
    "│   ├── 2019-01-01.csv\n",
    "│   ├── 2019-01-02.csv\n",
    "│   └── 2019-01-03.csv\n",
    "├── data_Q1_2018.zip\n",
    "├── data_Q1_2019.zip\n",
    "├── data_Q2_2018.zip\n",
    "├── data_Q2_2019.zip\n",
    "├── data_Q3_2018.zip\n",
    "├── data_Q3_2019.zip\n",
    "└── data_Q4_2018.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just in case they are passed as int\n",
    "years = [str(_) for _ in years]\n",
    "for y in years:\n",
    "    print(\"Year:\", y)\n",
    "    year_path = os.path.join(base_path, y)\n",
    "    os.makedirs(year_path, exist_ok=True)\n",
    "    for zip_name, unzip_dir in suffixes.items():\n",
    "        if y in zip_name:\n",
    "            url = base_url + zip_name\n",
    "            zip_path = os.path.join(base_path, zip_name)\n",
    "            if not os.path.exists(zip_path):\n",
    "                print(\"Downloading:\", url)\n",
    "                wget.download(url, out=base_path)\n",
    "            print(\"\\nUnzipping:\", zip_path)\n",
    "            dest_path = year_path if unzip_dir is None else base_path\n",
    "            with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "                z.extractall(dest_path)\n",
    "\n",
    "            if unzip_dir is not None and unzip_dir != y:\n",
    "                unzip_path = os.path.join(dest_path, unzip_dir)\n",
    "                for f in os.listdir(unzip_path):\n",
    "                    shutil.move(os.path.join(unzip_path, f),\n",
    "                            os.path.join(year_path, f))\n",
    "                os.rmdir(unzip_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all serial numbers of given HDD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_failed = []\n",
    "\n",
    "# for each year\n",
    "for year in years:\n",
    "    year_dir = year_dirs[year]\n",
    "    files = glob(os.path.join(year_dir, '*.csv'))\n",
    "\n",
    "    # for each file, or day\n",
    "    for file_path in sorted(files):\n",
    "        try:\n",
    "            file_r = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file {file_path} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # choose the HDD model we need\n",
    "        model_chosen = file_r[file_r['model'] == model]\n",
    "\n",
    "        # if that particular HDD model is not present, continue\n",
    "        if model_chosen.empty:\n",
    "            continue\n",
    "\n",
    "        # Print processing day\n",
    "        # print('processing day ' + str(model_chosen['date'].values))\n",
    "\n",
    "        if find_failed:\n",
    "            # choose only the failed hard drives\n",
    "            model_chosen = model_chosen[model_chosen['failure'] == 1]\n",
    "            # print(f\"Number of entries after filtering by failure: {len(model_chosen)}\")\n",
    "\n",
    "        # keep the failed hard drives' serial number\n",
    "        list_failed.extend(model_chosen['serial_number'].values)\n",
    "\n",
    "# Save the list of failed or all hard drives\n",
    "np.save(os.path.join(output_dir, f'HDD{years_list}_{suffix}_{model}.npy'), list_failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed = list_failed\n",
    "failed = set(np.load(os.path.join(output_dir, f'HDD{years_list}_{suffix}_{model}.npy')))\n",
    "\n",
    "database = pd.DataFrame()\n",
    "\n",
    "# Iterate over each year\n",
    "for year in years:\n",
    "    year_path = year_dirs[year]\n",
    "    files = sorted([f for f in os.listdir(year_path) if f.endswith('.csv')])\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for file in files:\n",
    "        file_path = os.path.join(year_path, file)\n",
    "        file_date = datetime.datetime.strptime(file.split('.')[0], '%Y-%m-%d')\n",
    "        old_time = datetime.datetime.strptime(f'{year}-01-01', '%Y-%m-%d')\n",
    "        \n",
    "        if file_date >= old_time:\n",
    "            df = pd.read_csv(file_path)\n",
    "            model_chosen = df[df['model'] == model]\n",
    "            relevant_rows = model_chosen[model_chosen['serial_number'].isin(failed)]\n",
    "\n",
    "            # Drop unnecessary columns since the following columns are not standard for all models\n",
    "            drop_columns = [col for col in relevant_rows if 'smart_' in col and int(col.split('_')[1]) in {22, 220, 222, 224, 226}]\n",
    "            relevant_rows.drop(columns=drop_columns, errors='ignore', inplace=True)\n",
    "\n",
    "            # Append the row to the database\n",
    "            database = pd.concat([database, relevant_rows], ignore_index=True)\n",
    "            print('adding day ' + str(model_chosen['date'].values))\n",
    "\n",
    "# Save the database to a pickle file\n",
    "database.to_pickle(os.path.join(output_dir, f'HDD{years_list}_{suffix}_{model}_appended.pkl'))\n",
    "\n",
    "# Check the most common models\n",
    "most_common_models = df.groupby(['model'], as_index=True)['model', 'date']. size()\n",
    "most_common_models = most_common_models.sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
